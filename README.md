# 知乎爬虫：爬取某一问题下的所有回答（回答数小于800左右）

## 项目简介

本项目是一个知乎爬虫工具，旨在爬取知乎某一问题下的所有回答（回答数小于800左右）。通过遍历问题ID，将符合条件的问题链接存入文件，并进行过滤后爬取所需的回答内容。

## 项目背景

知乎作为一个知识分享平台，拥有大量高质量的问答内容。然而，由于知乎的缓冲加载机制，当回答数量过多时（大约800左右），前面的回答信息可能无法完全抓取。本项目通过模拟滚动条操作，一次性抓取页面底端的所有回答元素，但当回答数量超过一定阈值时，前面的回答信息可能会丢失。

## 项目结构

```
│  config.py           # 爬取链接及存储路径设置
│  README.md
│  requirements.txt
│  scanner.py          # 获取有效的问题网址
│  filter_links.py     # 按照一定规则筛选问题
│  ZhihuSpider.py      # 知乎爬虫主程序
│
├─Driver
│      chromedriver.exe # Chrome驱动
│      geckodriver.exe  # gecko驱动
│
└─Results
        result-2022-07-28-深度神经网络DNN是否模拟了人类大脑皮层结构.csv # 抓取结果样例
```

## 功能模块

1. **config.py**: 配置文件，用于设置爬取链接及存储路径。
2. **scanner.py**: 获取有效的问题网址，并将其存入文件。
3. **filter_links.py**: 按照一定规则筛选问题，确保爬取的回答符合要求。
4. **ZhihuSpider.py**: 知乎爬虫主程序，负责爬取知乎问题下的回答内容。
5. **Driver**: 存放浏览器驱动程序，支持Chrome和Firefox浏览器。
6. **Results**: 存放爬取结果的文件夹，包含抓取结果样例。

## 使用方法

1. 配置`config.py`文件，设置爬取链接及存储路径。
2. 运行`scanner.py`，获取有效的问题网址并存入文件。
3. 运行`filter_links.py`，按照规则筛选问题。
4. 运行`ZhihuSpider.py`，开始爬取知乎问题下的回答内容。
5. 爬取结果将存储在`Results`文件夹中。

## 注意事项

- 由于知乎的缓冲加载机制，当回答数量超过800左右时，前面的回答信息可能无法完全抓取。
- 项目拟解决思路为边滚动边抓取，但目前尚未实现，因为不方便进行元素定位以避免重复抓取。

## 未来改进

- 实现边滚动边抓取的机制，以解决回答数量过多时信息丢失的问题。
- 优化元素定位方法，避免重复抓取。

## 结果样例

项目中包含一个抓取结果样例文件：`result-2022-07-28-深度神经网络DNN是否模拟了人类大脑皮层结构.csv`，展示了爬取结果的格式和内容。

## 贡献

欢迎大家提出改进建议或提交PR，共同完善这个知乎爬虫工具。

## 下载链接
[知乎爬虫爬取某一问题下的所有回答回答数小于800左右](https://pan.quark.cn/s/3ec9f5f1c8e2) 

(备用: [备用下载](https://pan.baidu.com/s/1hyXDnq01FQbMuujmXS3RtA?pwd=1234))

## 说明

该仓库仅用于学习交流，请勿用于商业用途。
